{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34c75db8-8428-47e5-b2d6-175ea987113c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.6-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.2-cp310-cp310-win_amd64.whl.metadata (111 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp310-cp310-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.6-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.6/8.1 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.4/8.1 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 5.8/8.1 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.3/8.1 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.9/8.1 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.9/8.1 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.2-cp310-cp310-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 1.0/2.3 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.3 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 5.1 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
      "Installing collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   ---------------- ----------------------- 2/5 [cycler]\n",
      "   ------------------------ --------------- 3/5 [contourpy]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   ---------------------------------------- 5/5 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.59.2 kiwisolver-1.4.9 matplotlib-3.10.6\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8c8d72f-28e7-46f9-8b3f-fbe32657fcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch-geometric 2.7.0\n",
      "Uninstalling torch-geometric-2.7.0:\n",
      "  Successfully uninstalled torch-geometric-2.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping torch-scatter as it is not installed.\n",
      "WARNING: Skipping torch-sparse as it is not installed.\n",
      "WARNING: Skipping torch-cluster as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.7.1+cu118.html\n",
      "Collecting torch-scatter\n",
      "  Downloading https://data.pyg.org/whl/torch-2.7.0%2Bcu118/torch_scatter-2.1.2%2Bpt27cu118-cp310-cp310-win_amd64.whl (3.6 MB)\n",
      "     ---------------------------------------- 0.0/3.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/3.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/3.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/3.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/3.6 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.3/3.6 MB ? eta -:--:--\n",
      "     -------- ------------------------------- 0.8/3.6 MB 1.8 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 1.0/3.6 MB 1.7 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 1.8/3.6 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 2.9/3.6 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 3.4/3.6 MB 3.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.6/3.6 MB 2.7 MB/s eta 0:00:00\n",
      "Installing collected packages: torch-scatter\n",
      "Successfully installed torch-scatter-2.1.2+pt27cu118\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.7.1+cu118.html\n",
      "Collecting torch-sparse\n",
      "  Downloading https://data.pyg.org/whl/torch-2.7.0%2Bcu118/torch_sparse-0.6.18%2Bpt27cu118-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "     -------------- ------------------------- 0.8/2.1 MB 1.8 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.0/2.1 MB 1.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.1/2.1 MB 2.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.1/2.1 MB 2.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.1/2.1 MB 2.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.1/2.1 MB 2.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.1/2.1 MB 2.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.1/2.1 MB 2.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 939.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch-sparse) (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from scipy->torch-sparse) (2.1.2)\n",
      "Installing collected packages: torch-sparse\n",
      "Successfully installed torch-sparse-0.6.18+pt27cu118\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.7.1+cu118.html\n",
      "Collecting torch-cluster\n",
      "  Downloading https://data.pyg.org/whl/torch-2.7.0%2Bcu118/torch_cluster-1.6.3%2Bpt27cu118-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "     ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 0.3/1.6 MB ? eta -:--:--\n",
      "     ------------------- -------------------- 0.8/1.6 MB 1.6 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 0.8/1.6 MB 1.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.6/1.6 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.6/1.6 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.6/1.6 MB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch-cluster) (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from scipy->torch-cluster) (2.1.2)\n",
      "Installing collected packages: torch-cluster\n",
      "Successfully installed torch-cluster-1.6.3+pt27cu118\n",
      "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
      "  Cloning https://github.com/pyg-team/pytorch_geometric.git to c:\\users\\windows\\appdata\\local\\temp\\pip-req-build-v5q09uga\n",
      "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit c139343e866ac35dff20434a3a75b829bf2f2083\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: aiohttp in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch-geometric==2.7.0) (3.12.15)\n",
      "Requirement already satisfied: fsspec in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch-geometric==2.7.0) (2024.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch-geometric==2.7.0) (3.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch-geometric==2.7.0) (2.1.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch-geometric==2.7.0) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch-geometric==2.7.0) (3.2.3)\n",
      "Requirement already satisfied: requests in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch-geometric==2.7.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch-geometric==2.7.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch-geometric==2.7.0) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->torch-geometric==2.7.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->torch-geometric==2.7.0) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->torch-geometric==2.7.0) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->torch-geometric==2.7.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->torch-geometric==2.7.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->torch-geometric==2.7.0) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->torch-geometric==2.7.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->torch-geometric==2.7.0) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric==2.7.0) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->torch-geometric==2.7.0) (3.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch-geometric==2.7.0) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->torch-geometric==2.7.0) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->torch-geometric==2.7.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->torch-geometric==2.7.0) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\windows\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm->torch-geometric==2.7.0) (0.4.6)\n",
      "Building wheels for collected packages: torch-geometric\n",
      "  Building wheel for torch-geometric (pyproject.toml): started\n",
      "  Building wheel for torch-geometric (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for torch-geometric: filename=torch_geometric-2.7.0-py3-none-any.whl size=1283608 sha256=f18aed90bc02bd578e37b4f5b774b771b583cb3b7bee045bbe959c3e44698064\n",
      "  Stored in directory: C:\\Users\\WINDOWS\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-fz58q75m\\wheels\\d3\\78\\eb\\9e26525b948d19533f1688fb6c209cec8a0ba793d39b49ae8f\n",
      "Successfully built torch-geometric\n",
      "Installing collected packages: torch-geometric\n",
      "Successfully installed torch-geometric-2.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git 'C:\\Users\\WINDOWS\\AppData\\Local\\Temp\\pip-req-build-v5q09uga'\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07a63c19-8088-40a3-b5f0-b08c1ea24a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "Dataset downloaded successfully!i = 1\n",
      "\n",
      "Epoch 0: Avg Loss=13.769181251525879, Alpha=0.5009999871253967, Beta=1.9989999532699585, Gamma=0.10100000351667404\n",
      "Epoch 50: Avg Loss=13.293166160583496, Alpha=0.5641430616378784, Beta=1.9489976167678833, Gamma=0.15014448761940002\n",
      "True classes: {0, 1, 2, 3, 4, 5, 6}\n",
      "Predicted classes: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(5), np.int64(6)}\n",
      "Plotting started!!\n",
      "Figure(800x600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"E:\\PhD\\Code\\YRS_CODS_25\\Rethinking_Modularity_Loss_with_Feature_Information_for_Better_Graph_Clustering\\train_with_imporved_mod.py\", line 109, in <module>\n",
      "    acc, nmi, ari, f1_macro = clustering_metrics(labels, pred_labels)\n",
      "  File \"E:\\PhD\\Code\\YRS_CODS_25\\Rethinking_Modularity_Loss_with_Feature_Information_for_Better_Graph_Clustering\\src\\utils.py\", line 96, in clustering_metrics\n",
      "    raise ValueError(\"Number of classes in true and predicted labels do not match!\")\n",
      "ValueError: Number of classes in true and predicted labels do not match!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully!\n",
      "Epoch 0: Avg Loss=13.76919937133789, Alpha=0.5009999871253967, Beta=1.9989999532699585, Gamma=0.10100000351667404\n",
      "Epoch 50: Avg Loss=13.294356346130371, Alpha=0.5640571117401123, Beta=1.9489976167678833, Gamma=0.15007488429546356\n",
      "True classes: {0, 1, 2, 3, 4, 5, 6}\n",
      "Predicted classes: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)}\n",
      "Plotting started!!\n",
      "Figure(800x600)\n",
      "Accuracy: 0.5753, NMI: 0.4464, ARI: 0.3813, F1 Macro: 0.3883\n",
      "i = 2\n",
      "Dataset downloaded successfully!\n",
      "Epoch 0: Avg Loss=13.769307136535645, Alpha=0.5009999871253967, Beta=1.9989999532699585, Gamma=0.10100000351667404\n",
      "Epoch 50: Avg Loss=13.295295715332031, Alpha=0.564009428024292, Beta=1.9489976167678833, Gamma=0.15022887289524078\n",
      "True classes: {0, 1, 2, 3, 4, 5, 6}\n",
      "Predicted classes: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)}\n",
      "Plotting started!!\n",
      "Figure(800x600)\n",
      "Accuracy: 0.5114, NMI: 0.3749, ARI: 0.3125, F1 Macro: 0.4358\n",
      "i = 3\n",
      "Dataset downloaded successfully!i = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"E:\\PhD\\Code\\YRS_CODS_25\\Rethinking_Modularity_Loss_with_Feature_Information_for_Better_Graph_Clustering\\train_with_imporved_mod.py\", line 109, in <module>\n",
      "    acc, nmi, ari, f1_macro = clustering_metrics(labels, pred_labels)\n",
      "  File \"E:\\PhD\\Code\\YRS_CODS_25\\Rethinking_Modularity_Loss_with_Feature_Information_for_Better_Graph_Clustering\\src\\utils.py\", line 96, in clustering_metrics\n",
      "    raise ValueError(\"Number of classes in true and predicted labels do not match!\")\n",
      "ValueError: Number of classes in true and predicted labels do not match!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: Avg Loss=13.769194602966309, Alpha=0.5009999871253967, Beta=1.9989999532699585, Gamma=0.10100000351667404\n",
      "Epoch 50: Avg Loss=13.296740531921387, Alpha=0.5640532970428467, Beta=1.9489976167678833, Gamma=0.1500866413116455\n",
      "True classes: {0, 1, 2, 3, 4, 5, 6}\n",
      "Predicted classes: {np.int64(0), np.int64(2), np.int64(4), np.int64(5), np.int64(6)}\n",
      "Plotting started!!\n",
      "Figure(800x600)\n",
      "Dataset downloaded successfully!\n",
      "Epoch 0: Avg Loss=13.769229888916016, Alpha=0.5009999871253967, Beta=1.9989999532699585, Gamma=0.10100000351667404\n",
      "Epoch 50: Avg Loss=13.301302909851074, Alpha=0.5642176270484924, Beta=1.9489976167678833, Gamma=0.15019723773002625\n",
      "True classes: {0, 1, 2, 3, 4, 5, 6}\n",
      "Predicted classes: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)}\n",
      "Plotting started!!\n",
      "Figure(800x600)\n",
      "Accuracy: 0.4435, NMI: 0.3970, ARI: 0.2512, F1 Macro: 0.3911\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"i =\", i)\n",
    "    !python train_with_imporved_mod.py  ## Cora imporved modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c948c30-55fd-466b-91a1-22968162b089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "Dataset downloaded successfully!i = 1\n",
      "\n",
      "Epoch 0: Avg Loss=11.799118041992188, Alpha=0.5009999871253967, Beta=1.9989999532699585, Gamma=0.10100000351667404\n",
      "Epoch 50: Avg Loss=11.319223403930664, Alpha=0.5633454918861389, Beta=1.9489976167678833, Gamma=0.14833803474903107\n",
      "True classes: {0, 1, 2, 3, 4, 5}\n",
      "Predicted classes: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)}\n",
      "Plotting started!!\n",
      "Figure(800x600)\n",
      "Accuracy: 0.5044, NMI: 0.2770, ARI: 0.2573, F1 Macro: 0.4446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PhD\\Code\\YRS_CODS_25\\Rethinking_Modularity_Loss_with_Feature_Information_for_Better_Graph_Clustering\\src\\utils.py:48: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(row_sum, -0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully!i = 2\n",
      "\n",
      "Epoch 0: Avg Loss=11.799089431762695, Alpha=0.5009999871253967, Beta=1.9989999532699585, Gamma=0.10100000351667404\n",
      "Epoch 50: Avg Loss=11.31000804901123, Alpha=0.5626766085624695, Beta=1.9489976167678833, Gamma=0.1462043821811676\n",
      "True classes: {0, 1, 2, 3, 4, 5}\n",
      "Predicted classes: {np.int64(0), np.int64(2), np.int64(3), np.int64(4), np.int64(5)}\n",
      "Plotting started!!\n",
      "Figure(800x600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PhD\\Code\\YRS_CODS_25\\Rethinking_Modularity_Loss_with_Feature_Information_for_Better_Graph_Clustering\\src\\utils.py:48: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(row_sum, -0.5)\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\PhD\\Code\\YRS_CODS_25\\Rethinking_Modularity_Loss_with_Feature_Information_for_Better_Graph_Clustering\\train_with_imporved_mod.py\", line 109, in <module>\n",
      "    acc, nmi, ari, f1_macro = clustering_metrics(labels, pred_labels)\n",
      "  File \"E:\\PhD\\Code\\YRS_CODS_25\\Rethinking_Modularity_Loss_with_Feature_Information_for_Better_Graph_Clustering\\src\\utils.py\", line 96, in clustering_metrics\n",
      "    raise ValueError(\"Number of classes in true and predicted labels do not match!\")\n",
      "ValueError: Number of classes in true and predicted labels do not match!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully!\n",
      "Epoch 0: Avg Loss=11.799108505249023, Alpha=0.5009999871253967, Beta=1.9989999532699585, Gamma=0.10100000351667404\n",
      "Epoch 50: Avg Loss=11.298819541931152, Alpha=0.5628235936164856, Beta=1.9489976167678833, Gamma=0.1471286118030548\n",
      "True classes: {0, 1, 2, 3, 4, 5}\n",
      "Predicted classes: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)}\n",
      "Plotting started!!\n",
      "Figure(800x600)\n",
      "Accuracy: 0.6201, NMI: 0.3773, ARI: 0.3813, F1 Macro: 0.5793\n",
      "i = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PhD\\Code\\YRS_CODS_25\\Rethinking_Modularity_Loss_with_Feature_Information_for_Better_Graph_Clustering\\src\\utils.py:48: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(row_sum, -0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully!i = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PhD\\Code\\YRS_CODS_25\\Rethinking_Modularity_Loss_with_Feature_Information_for_Better_Graph_Clustering\\src\\utils.py:48: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(row_sum, -0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: Avg Loss=11.799077033996582, Alpha=0.5009999871253967, Beta=1.9989999532699585, Gamma=0.10100000351667404\n",
      "Epoch 50: Avg Loss=11.30932331085205, Alpha=0.5631797313690186, Beta=1.9489976167678833, Gamma=0.14779038727283478\n",
      "True classes: {0, 1, 2, 3, 4, 5}\n",
      "Predicted classes: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)}\n",
      "Plotting started!!\n",
      "Figure(800x600)\n",
      "Accuracy: 0.6513, NMI: 0.3545, ARI: 0.3671, F1 Macro: 0.5649\n",
      "Dataset downloaded successfully!\n",
      "Epoch 0: Avg Loss=11.799074172973633, Alpha=0.5009999871253967, Beta=1.9989999532699585, Gamma=0.10100000351667404\n",
      "Epoch 50: Avg Loss=11.304610252380371, Alpha=0.5625242590904236, Beta=1.9489976167678833, Gamma=0.14618124067783356\n",
      "True classes: {0, 1, 2, 3, 4, 5}\n",
      "Predicted classes: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)}\n",
      "Plotting started!!\n",
      "Figure(800x600)\n",
      "Accuracy: 0.3940, NMI: 0.2278, ARI: 0.1811, F1 Macro: 0.3299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PhD\\Code\\YRS_CODS_25\\Rethinking_Modularity_Loss_with_Feature_Information_for_Better_Graph_Clustering\\src\\utils.py:48: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(row_sum, -0.5)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"i =\", i)\n",
    "    !python train_with_imporved_mod.py  ## Citeseer imporved modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af56d749-7c34-4e94-ac47-cf6e1020ff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully!\n",
      "Epoch 0: Avg Loss=5.086179010279767, Alpha=0.9576418995857239, Beta=1.6920021772384644, Gamma=0.2605501115322113\n",
      "Epoch 1: Avg Loss=3.911685950570292, Alpha=1.323563575744629, Beta=1.3840035200119019, Gamma=0.32991066575050354\n",
      "Epoch 2: Avg Loss=2.8349283097626325, Alpha=1.6649571657180786, Beta=1.0760043859481812, Gamma=0.39314401149749756\n",
      "Epoch 3: Avg Loss=1.7663424808483619, Alpha=1.9959380626678467, Beta=0.7680065035820007, Gamma=0.4549536406993866\n",
      "Epoch 4: Avg Loss=0.7015251565434328, Alpha=2.320786476135254, Beta=0.4600086510181427, Gamma=0.5175827741622925\n",
      "Epoch 5: Avg Loss=-0.3600904921208835, Alpha=2.6414051055908203, Beta=0.1520102620124817, Gamma=0.5831292867660522\n",
      "Epoch 6: Avg Loss=-1.421210077288863, Alpha=2.9593849182128906, Beta=-0.1559879034757614, Gamma=0.6508963704109192\n",
      "Epoch 7: Avg Loss=-2.481706878194561, Alpha=3.275377035140991, Beta=-0.4639859199523926, Gamma=0.7235342264175415\n",
      "Epoch 8: Avg Loss=-3.541483041527983, Alpha=3.589770555496216, Beta=-0.7719834446907043, Gamma=0.802474319934845\n",
      "Epoch 9: Avg Loss=-4.599881282874516, Alpha=3.902956485748291, Beta=-1.0799813270568848, Gamma=0.8855714797973633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"E:\\PhD\\Code\\YRS_CODS_25\\Rethinking_Modularity_Loss_with_Feature_Information_for_Better_Graph_Clustering\\train_with_imporved_mod.py\", line 93, in <module>\n",
      "    out = model(features, edge_index)\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"E:\\PhD\\Code\\YRS_CODS_25\\Rethinking_Modularity_Loss_with_Feature_Information_for_Better_Graph_Clustering\\src\\models\\gcn_and_sage.py\", line 31, in forward\n",
      "    x = self.sage1(x, edge_index)\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\nn\\conv\\sage_conv.py\", line 134, in forward\n",
      "    out = self.propagate(edge_index, x=x, size=size)\n",
      "  File \"C:\\Users\\WINDOWS\\AppData\\Local\\Temp\\torch_geometric.nn.conv.sage_conv_SAGEConv_propagate_y0kjw_0i.py\", line 229, in propagate\n",
      "    out = self.aggregate(\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\", line 594, in aggregate\n",
      "    return self.aggr_module(inputs, index, ptr=ptr, dim_size=dim_size,\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\experimental.py\", line 117, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\nn\\aggr\\base.py\", line 139, in __call__\n",
      "    raise e\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\nn\\aggr\\base.py\", line 131, in __call__\n",
      "    return super().__call__(x, index=index, ptr=ptr, dim_size=dim_size,\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\nn\\aggr\\basic.py\", line 36, in forward\n",
      "    return self.reduce(x, index, ptr, dim_size, dim, reduce='mean')\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\nn\\aggr\\base.py\", line 185, in reduce\n",
      "    return scatter(x, index, dim, dim_size, reduce)\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\utils\\_scatter.py\", line 80, in scatter\n",
      "    return out / broadcast(count, out, dim)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 2.00 GiB of which 0 bytes is free. Of the allocated memory 1.71 GiB is allocated by PyTorch, and 4.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "!python train_with_imporved_mod.py  ## Pubmed imporved modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d33e3252-fc30-4f06-9ba2-3406dae1921d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully!\n",
      "Nodes in current batch:  [8193 8197   12 ... 8162 8168 8190]\n",
      "Nodes in current batch:  [8194   14   15 ... 8182 8184 8190]\n",
      "Nodes in current batch:  [   1 8194    3 ... 8186 8187 8190]\n",
      "Nodes in current batch:  [   1    7 8203 ... 8171 8182 8184]\n",
      "Nodes in current batch:  [   2    3    7 ... 8178 8184 8188]\n",
      "Nodes in current batch:  [8197    7 8199 ... 8177 8186 8188]\n",
      "Nodes in current batch:  [   1    3 8201 ... 8178 8184 8190]\n",
      "Nodes in current batch:  [   1 8196    5 ... 8178 8181 8190]\n",
      "Nodes in current batch:  [   1 8193 8197 ... 8184 8188 8191]\n",
      "Nodes in current batch:  [   1 8194    3 ... 8178 8182 8185]\n",
      "Nodes in current batch:  [   1 8202 8203 ... 8170 8173 8176]\n",
      "Nodes in current batch:  [8197    9 8201 ... 8182 8183 8188]\n",
      "Nodes in current batch:  [   2 8197    5 ... 8176 8177 8184]\n",
      "Nodes in current batch:  [   1 8194 8201 ... 8178 8180 8182]\n",
      "Nodes in current batch:  [8197 8199   12 ... 8170 8178 8182]\n",
      "Nodes in current batch:  [   7    8    9 ... 8176 8178 8186]\n",
      "Nodes in current batch:  [8194    3 8197 ... 8182 8184 8190]\n",
      "Nodes in current batch:  [   1 8194    7 ... 8180 8182 8183]\n",
      "Nodes in current batch:  [8197 8201   12 ... 8181 8182 8190]\n",
      "Nodes in current batch:  [8195 8196 8200 ... 8176 8178 8191]\n",
      "Nodes in current batch:  [8193    3 8199 ... 8184 8190 8191]\n",
      "Nodes in current batch:  [8194    3    2 ... 8184 8186 8190]\n",
      "Nodes in current batch:  [   0 8194    3 ... 8173 8184 8190]\n",
      "Nodes in current batch:  [   7 8199 8203 ... 8184 8188 8191]\n",
      "Nodes in current batch:  [   1 8194    3 ... 8171 8173 8184]\n",
      "Nodes in current batch:  [   1 8194    8 ... 8180 8184 8187]\n",
      "Nodes in current batch:  [8194 8195    3 ... 8178 8180 8184]\n",
      "Nodes in current batch:  [8192    3 8197 ... 8170 8176 8178]\n",
      "Nodes in current batch:  [8192   12   16 ... 8182 8184 8186]\n",
      "Nodes in current batch:  [   1    3 8195 ... 8170 8178 8184]\n",
      "Nodes in current batch:  [   2    3 8196 ... 8181 8182 8184]\n",
      "Nodes in current batch:  [8194    3 8196 ... 8173 8176 8177]\n",
      "Nodes in current batch:  [8194    3 8197 ... 8184 8186 8188]\n",
      "Nodes in current batch:  [8201 8203   12 ... 8180 8182 8184]\n",
      "Nodes in current batch:  [8194    3 8197 ... 8178 8186 8191]\n",
      "Nodes in current batch:  [   1 8196 8199 ... 8180 8186 8188]\n",
      "Nodes in current batch:  [8192    1    2 ... 8180 8181 8187]\n",
      "Nodes in current batch:  [8194 8195    3 ... 8173 8184 8190]\n",
      "Nodes in current batch:  [8192 8193 8194 ... 8176 8182 8191]\n",
      "Nodes in current batch:  [8193    2    3 ... 8170 8182 8184]\n",
      "Nodes in current batch:  [   1 8199 8201 ... 8170 8176 8182]\n",
      "Nodes in current batch:  [8194    5 8201 ... 8178 8181 8184]\n",
      "Nodes in current batch:  [   0 8194 8195 ... 8173 8180 8188]\n",
      "Nodes in current batch:  [   1    3 8200 ... 8182 8184 8190]\n",
      "Nodes in current batch:  [8194    3 8201 ... 8157 8159 8180]\n",
      "Nodes in current batch:  [   1    2 8194 ... 8173 8178 8180]\n",
      "Nodes in current batch:  [   1    3 8197 ... 8188 8190 8191]\n",
      "Nodes in current batch:  [8197 8201 8203 ... 8180 8182 8184]\n",
      "Nodes in current batch:  [   1    2    3 ... 8178 8180 8182]\n",
      "Nodes in current batch:  [   3    4 8199 ... 8170 8172 8190]\n",
      "Nodes in current batch:  [   3 8201   16 ... 8173 8184 8190]\n",
      "Nodes in current batch:  [   3 8196   11 ... 8182 8184 8190]\n",
      "Nodes in current batch:  [8193    2    3 ... 8178 8183 8184]\n",
      "Nodes in current batch:  [8192 8200 8201 ... 8178 8181 8184]\n",
      "Nodes in current batch:  [   3    6 8200 ... 8182 8184 8186]\n",
      "Nodes in current batch:  [   3 8199    8 ... 8182 8188 8190]\n",
      "Nodes in current batch:  [   1    3    6 ... 8176 8178 8183]\n",
      "Nodes in current batch:  [   3 8197 8201 ... 8177 8181 8184]\n",
      "Nodes in current batch:  [8193    2 8197 ... 8184 8188 8190]\n",
      "Nodes in current batch:  [   1    3 8196 ... 8178 8182 8187]\n",
      "Nodes in current batch:  [   0 8194 8195 ... 8178 8180 8190]\n",
      "Nodes in current batch:  [   1 8194    3 ... 8173 8175 8180]\n",
      "Nodes in current batch:  [8194    4 8201 ... 8178 8184 8190]\n",
      "Nodes in current batch:  [8194    3    2 ... 8168 8184 8190]\n",
      "Nodes in current batch:  [   1    3 8196 ... 8172 8176 8180]\n",
      "Nodes in current batch:  [8192    2    3 ... 8161 8170 8178]\n",
      "Nodes in current batch:  [8193    5 8200 ... 8182 8186 8190]\n",
      "Nodes in current batch:  [   0 8194 8201 ... 8171 8172 8173]\n",
      "Nodes in current batch:  [   2    3    7 ... 8168 8171 8182]\n",
      "Nodes in current batch:  [   1 8195    3 ... 8162 8168 8178]\n",
      "Nodes in current batch:  [8192    1    3 ... 8178 8180 8186]\n",
      "Nodes in current batch:  [   1 8194 8195 ... 8178 8183 8184]\n",
      "Nodes in current batch:  [8194    3 8197 ... 8170 8173 8176]\n",
      "Nodes in current batch:  [8194    3    5 ... 8182 8184 8190]\n",
      "Nodes in current batch:  [8201 8203   16 ... 8173 8178 8190]\n",
      "Nodes in current batch:  [   5    7 8200 ... 8173 8176 8188]\n",
      "Nodes in current batch:  [8197   11 8203 ... 8184 8188 8190]\n",
      "Nodes in current batch:  [   1 8194    3 ... 8184 8188 8190]\n",
      "Nodes in current batch:  [   1    5    7 ... 8176 8180 8183]\n",
      "Nodes in current batch:  [8197 8200 8201 ... 8178 8182 8190]\n",
      "Nodes in current batch:  [   0    1    3 ... 8188 8190 8191]\n",
      "Nodes in current batch:  [8194    5 8197 ... 8167 8177 8178]\n",
      "Nodes in current batch:  [8194    3 8199 ... 8177 8183 8188]\n",
      "Nodes in current batch:  [   1    3    5 ... 8182 8184 8185]\n",
      "Nodes in current batch:  [   1    3    7 ... 8178 8180 8186]\n",
      "Nodes in current batch:  [   3 8201   12 ... 8182 8183 8185]\n",
      "Nodes in current batch:  [8194 8195   12 ... 8180 8184 8190]\n",
      "Nodes in current batch:  [8194    3    7 ... 8173 8176 8184]\n",
      "Nodes in current batch:  [   2    3 8200 ... 8178 8182 8184]\n",
      "Nodes in current batch:  [8194    7 8201 ... 8173 8182 8190]\n",
      "Nodes in current batch:  [   1 8193    3 ... 8158 8162 8168]\n",
      "Nodes in current batch:  [8192    1 8193 ... 8178 8184 8190]\n",
      "Nodes in current batch:  [8192    1    2 ... 8175 8180 8184]\n",
      "Nodes in current batch:  [   1 8195    5 ... 8184 8185 8191]\n",
      "Nodes in current batch:  [   1 8193 8199 ... 8182 8184 8186]\n",
      "Nodes in current batch:  [   3 8201   16 ... 8187 8188 8190]\n",
      "Nodes in current batch:  [8194    3    5 ... 8177 8180 8186]\n",
      "Nodes in current batch:  [8197   11   12 ... 8180 8184 8188]\n",
      "Nodes in current batch:  [   2 8201   12 ... 8180 8182 8184]\n",
      "Nodes in current batch:  [8194    3 8197 ... 8182 8184 8185]\n",
      "Nodes in current batch:  [   1    3 8197 ... 8182 8184 8186]\n",
      "Nodes in current batch:  [8193 8194 8196 ... 8176 8180 8186]\n",
      "Nodes in current batch:  [   1    3 8197 ... 8178 8181 8184]\n",
      "Nodes in current batch:  [8194    3 8196 ... 8173 8176 8183]\n",
      "Nodes in current batch:  [8199 8201 8204 ... 8162 8181 8184]\n",
      "Nodes in current batch:  [8192    1 8194 ... 8168 8182 8186]\n",
      "Nodes in current batch:  [   2 8196 8199 ... 8188 8190 8191]\n",
      "Nodes in current batch:  [8194    2 8196 ... 8172 8176 8184]\n",
      "Nodes in current batch:  [   3    5 8199 ... 8181 8182 8190]\n",
      "Nodes in current batch:  [   1    2 8197 ... 8184 8189 8190]\n",
      "Nodes in current batch:  [8200   11   12 ... 8169 8176 8178]\n",
      "Nodes in current batch:  [8193    3    5 ... 8173 8176 8180]\n",
      "Nodes in current batch:  [   1    2 8199 ... 8173 8175 8176]\n",
      "Nodes in current batch:  [   3    7   12 ... 8180 8186 8191]\n",
      "Nodes in current batch:  [8192 8207 8208 ... 8180 8182 8184]\n",
      "Nodes in current batch:  [   2 8201 8203 ... 8176 8182 8187]\n",
      "Nodes in current batch:  [   3 8197 8201 ... 8175 8184 8186]\n",
      "Nodes in current batch:  [8193    2    3 ... 8173 8184 8188]\n",
      "Nodes in current batch:  [8194    8   11 ... 8182 8184 8188]\n",
      "Nodes in current batch:  [   1    3    7 ... 8178 8182 8184]\n",
      "Nodes in current batch:  [8193 8194 8196 ... 8184 8188 8190]\n",
      "Nodes in current batch:  [   1    7 8201 ... 8182 8183 8191]\n",
      "Nodes in current batch:  [   7 8201   15 ... 8176 8180 8190]\n",
      "Nodes in current batch:  [   3    7 8203 ... 8168 8170 8178]\n",
      "Nodes in current batch:  [8193    3    9 ... 8184 8186 8188]\n",
      "Nodes in current batch:  [   1 8195    3 ... 8178 8184 8188]\n",
      "Nodes in current batch:  [   1 8194 8199 ... 8180 8181 8186]\n",
      "Nodes in current batch:  [   3 8196    7 ... 8182 8186 8191]\n",
      "Nodes in current batch:  [8194    3    5 ... 8176 8181 8184]\n",
      "Nodes in current batch:  [   1    3    7 ... 8168 8184 8190]\n",
      "Nodes in current batch:  [8199 8201 8205 ... 8178 8181 8184]\n",
      "Nodes in current batch:  [8194    3 8201 ... 8176 8181 8184]\n",
      "Nodes in current batch:  [   3 8199 8201 ... 8166 8173 8190]\n",
      "Nodes in current batch:  [8194    2 8197 ... 8177 8180 8190]\n",
      "Nodes in current batch:  [   1    3    5 ... 8180 8182 8184]\n",
      "Nodes in current batch:  [   1 8196 8206 ... 8182 8184 8185]\n",
      "Nodes in current batch:  [8195 8201 8203 ... 8180 8184 8188]\n",
      "Nodes in current batch:  [8192    1 8197 ... 8184 8188 8190]\n",
      "Nodes in current batch:  [8197 8198 8199 ... 8176 8178 8181]\n",
      "Nodes in current batch:  [   0 8192    5 ... 8176 8181 8184]\n",
      "Nodes in current batch:  [8193 8201   11 ... 8180 8185 8190]\n",
      "Nodes in current batch:  [8192 8201   16 ... 8181 8182 8184]\n",
      "Nodes in current batch:  [   2 8194    3 ... 8184 8186 8187]\n",
      "Nodes in current batch:  [   1 8194    7 ... 8176 8180 8184]\n",
      "Nodes in current batch:  [8194 8197    8 ... 8178 8182 8190]\n",
      "Nodes in current batch:  [   1 8194    3 ... 8178 8183 8184]\n",
      "Nodes in current batch:  [   2 8195 8197 ... 8170 8172 8191]\n",
      "Nodes in current batch:  [8192    2 8195 ... 8172 8180 8188]\n",
      "Nodes in current batch:  [8192    2 8197 ... 8181 8190 8191]\n",
      "Nodes in current batch:  [8192 8194 8195 ... 8172 8184 8188]\n",
      "Nodes in current batch:  [   1    2    3 ... 8178 8184 8188]\n",
      "Nodes in current batch:  [   7 8201   14 ... 8180 8182 8184]\n",
      "Nodes in current batch:  [8192   12   14 ... 8178 8182 8190]\n",
      "Nodes in current batch:  [8194    3    5 ... 8180 8186 8190]\n",
      "Nodes in current batch:  [8197 8199 8201 ... 8178 8182 8184]\n",
      "Nodes in current batch:  [   1 8194    3 ... 8169 8177 8180]\n",
      "Nodes in current batch:  [   0    3    5 ... 8178 8181 8182]\n",
      "Nodes in current batch:  [8192    3    5 ... 8176 8184 8190]\n",
      "Nodes in current batch:  [8194 8195    6 ... 8184 8190 8191]\n",
      "Nodes in current batch:  [   1    2    3 ... 8177 8178 8182]\n",
      "Nodes in current batch:  [8192 8193 8197 ... 8181 8184 8190]\n",
      "Nodes in current batch:  [8194    5    7 ... 8188 8190 8191]\n",
      "Nodes in current batch:  [   1 8194    3 ... 8180 8182 8188]\n",
      "Nodes in current batch:  [   1 8194 8200 ... 8178 8184 8186]\n",
      "Nodes in current batch:  [   1    2 8197 ... 8176 8180 8187]\n",
      "Nodes in current batch:  [   1 8194 8197 ... 8178 8184 8190]\n",
      "Nodes in current batch:  [   1    3 8201 ... 8168 8170 8182]\n",
      "Nodes in current batch:  [8192 8194    6 ... 8180 8182 8190]\n",
      "Nodes in current batch:  [8192    5 8210 ... 8178 8183 8184]\n",
      "Nodes in current batch:  [   3    5   15 ... 8178 8182 8188]\n",
      "Nodes in current batch:  [   3 8203 8204 ... 8173 8174 8184]\n",
      "Nodes in current batch:  [8193 8201   16 ... 8178 8184 8188]\n",
      "Nodes in current batch:  [8194    5 8197 ... 8174 8184 8187]\n",
      "Nodes in current batch:  [   1    7 8201 ... 8176 8180 8188]\n",
      "Nodes in current batch:  [   1 8194 8201 ... 8180 8184 8186]\n",
      "Nodes in current batch:  [   1 8194    2 ... 8177 8184 8188]\n",
      "Nodes in current batch:  [8194 8197 8201 ... 8188 8190 8191]\n",
      "Nodes in current batch:  [   5 8203   11 ... 8181 8182 8190]\n",
      "Nodes in current batch:  [   1 8194    3 ... 8182 8183 8187]\n",
      "Nodes in current batch:  [   0    1 8194 ... 8159 8162 8173]\n",
      "Nodes in current batch:  [8197    8 8201 ... 8173 8174 8190]\n",
      "Nodes in current batch:  [   2    3 8201 ... 8178 8182 8184]\n",
      "Nodes in current batch:  [   5    7 8201 ... 8178 8181 8184]\n",
      "Nodes in current batch:  [8197    5    7 ... 8177 8180 8191]\n",
      "Nodes in current batch:  [8194    3 8200 ... 8180 8184 8191]\n",
      "Nodes in current batch:  [   0 8196    5 ... 8177 8180 8184]\n",
      "Nodes in current batch:  [   1 8195    3 ... 8174 8182 8190]\n",
      "Nodes in current batch:  [8193    1 8197 ... 8182 8186 8190]\n",
      "Nodes in current batch:  [   1    2    3 ... 8168 8183 8191]\n",
      "Nodes in current batch:  [   2    3    5 ... 8169 8177 8188]\n",
      "Nodes in current batch:  [8194    3 8195 ... 8157 8162 8174]\n",
      "Nodes in current batch:  [8196 8197 8201 ... 8178 8186 8190]\n",
      "Nodes in current batch:  [8194 8200 8203 ... 8176 8178 8182]\n",
      "Nodes in current batch:  [   3    5 8201 ... 8181 8186 8190]\n",
      "Nodes in current batch:  [8193    3 8197 ... 8182 8184 8186]\n",
      "Nodes in current batch:  [   1 8194    2 ... 8186 8190 8191]\n",
      "Nodes in current batch:  [   0 8192    6 ... 8178 8181 8186]\n",
      "Nodes in current batch:  [8196 8201 8203 ... 8170 8172 8178]\n",
      "Nodes in current batch:  [   3 8197    7 ... 8180 8182 8186]\n",
      "Nodes in current batch:  [   1    3 8197 ... 8180 8182 8185]\n",
      "Nodes in current batch:  [   7   15   16 ... 8170 8180 8186]\n",
      "Nodes in current batch:  [8193 8194    3 ... 8176 8178 8188]\n",
      "Nodes in current batch:  [   1    3 8201 ... 8175 8176 8180]\n",
      "Nodes in current batch:  [8193    2    3 ... 8175 8177 8184]\n",
      "Nodes in current batch:  [8194    5 8198 ... 8172 8173 8178]\n",
      "Nodes in current batch:  [   2    3   11 ... 8184 8187 8188]\n",
      "Nodes in current batch:  [   1    3 8197 ... 8178 8184 8190]\n",
      "Nodes in current batch:  [8194   12   15 ... 8170 8173 8178]\n",
      "Nodes in current batch:  [   1 8195 8196 ... 8182 8184 8186]\n",
      "Nodes in current batch:  [8194    5    6 ... 8184 8186 8188]\n",
      "Nodes in current batch:  [   1   11   12 ... 8180 8182 8190]\n",
      "Nodes in current batch:  [8194    7 8201 ... 8176 8181 8184]\n",
      "Nodes in current batch:  [   4 8197    5 ... 8180 8184 8190]\n",
      "Nodes in current batch:  [8192    1    3 ... 8177 8178 8186]\n",
      "Epoch 0: Avg Loss=18.395562635403927, Alpha=0.5410430431365967, Beta=1.7859927415847778, Gamma=0.31450310349464417\n",
      "Epoch 1: Avg Loss=15.768785579182277, Alpha=0.5544334650039673, Beta=1.5719879865646362, Gamma=0.5287124514579773\n",
      "Epoch 2: Avg Loss=13.143535716511378, Alpha=0.6139704585075378, Beta=1.357983946800232, Gamma=0.7427534461021423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PhD\\Code\\YRS_CODS_25\\Rethinking_Modularity_Loss_with_Feature_Information_for_Better_Graph_Clustering\\src\\utils.py:48: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(row_sum, -0.5)\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\PhD\\Code\\YRS_CODS_25\\Rethinking_Modularity_Loss_with_Feature_Information_for_Better_Graph_Clustering\\train_with_imporved_mod.py\", line 95, in <module>\n",
      "    out = model(features, edge_index)\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"E:\\PhD\\Code\\YRS_CODS_25\\Rethinking_Modularity_Loss_with_Feature_Information_for_Better_Graph_Clustering\\src\\models\\gcn_and_sage.py\", line 31, in forward\n",
      "    x = self.sage1(x, edge_index)\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\nn\\conv\\sage_conv.py\", line 134, in forward\n",
      "    out = self.propagate(edge_index, x=x, size=size)\n",
      "  File \"C:\\Users\\WINDOWS\\AppData\\Local\\Temp\\torch_geometric.nn.conv.sage_conv_SAGEConv_propagate_yznjgj0v.py\", line 173, in propagate\n",
      "    kwargs = self.collect(\n",
      "  File \"C:\\Users\\WINDOWS\\AppData\\Local\\Temp\\torch_geometric.nn.conv.sage_conv_SAGEConv_propagate_yznjgj0v.py\", line 83, in collect\n",
      "    x_j = self._index_select(_x_0, edge_index_j)\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\", line 267, in _index_select\n",
      "    return self._index_select_safe(src, index)\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\", line 290, in _index_select_safe\n",
      "    raise e\n",
      "  File \"C:\\Users\\WINDOWS\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py\", line 271, in _index_select_safe\n",
      "    return src.index_select(self.node_dim, index)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.41 GiB. GPU 0 has a total capacity of 2.00 GiB of which 26.48 MiB is free. Of the allocated memory 800.79 MiB is allocated by PyTorch, and 301.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "!python train_with_imporved_mod.py  ## Computers imporved modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d85bc-00a1-4a3a-81b9-13d881b55f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
