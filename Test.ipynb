{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5d34e70-13db-44f5-b5e9-1132444848ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]])\n",
      "X: tensor([[1., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]])\n",
      "Adjacency matrix: [[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Dummy data\n",
    "n_nodes = 4\n",
    "n_clusters = 2\n",
    "\n",
    "# Cluster assignment matrix C: each node belongs fully to one cluster (one-hot)\n",
    "C = torch.tensor([\n",
    "    [1, 0],  # Node 0 in cluster 0\n",
    "    [1, 0],  # Node 1 in cluster 0\n",
    "    [0, 1],  # Node 2 in cluster 1\n",
    "    [0, 1],  # Node 3 in cluster 1\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Node features matrix X: simple 2D features\n",
    "X = torch.tensor([\n",
    "    [1, 0],  # Node 0 feature\n",
    "    [1, 0],  # Node 1 feature\n",
    "    [0, 1],  # Node 2 feature\n",
    "    [0, 1],  # Node 3 feature\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Adjacency matrix for a small graph (4 nodes)\n",
    "adj = np.array([\n",
    "    [0, 1, 0, 0],  # Node 0 connected to Node 1\n",
    "    [1, 0, 0, 0],  # Node 1 connected to Node 0\n",
    "    [0, 0, 0, 1],  # Node 2 connected to Node 3\n",
    "    [0, 0, 1, 0],  # Node 3 connected to Node 2\n",
    "], dtype=np.float32)\n",
    "\n",
    "edge_index = torch.tensor([\n",
    "    [0, 1, 2, 3],  # source nodes\n",
    "    [1, 0, 3, 2]   # target nodes\n",
    "], dtype=torch.long)\n",
    "\n",
    "# The expected:\n",
    "# - Nodes are split evenly into 2 clusters: first 2 nodes in cluster 0, last 2 in cluster 1\n",
    "# - Adjacency consists of two disconnected components (node 0-1 and node 2-3)\n",
    "# - Features match the cluster assignment perfectly (cluster 0 nodes have feature [1,0], cluster 1 have [0,1])\n",
    "# This should give us a high modularity score and low loss.\n",
    "\n",
    "print(\"C:\", C)\n",
    "print(\"X:\", X)\n",
    "print(\"Adjacency matrix:\", adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2807a794-ed21-47ca-85af-fc134dd5e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModularityLoss(nn.Module):\n",
    "    def __init__(self, n_clusters, initial_alpha=0.5):\n",
    "        super(ModularityLoss, self).__init__()\n",
    "        # Make alpha a learnable parameter\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = nn.Parameter(torch.tensor(initial_alpha, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, C, X, adj):\n",
    "        device = C.device\n",
    "        n = adj.shape[0]\n",
    "        k = self.n_clusters\n",
    "        # Convert adjacency matrix to torch tensor on same device\n",
    "        if not torch.is_tensor(adj):\n",
    "            adj_tensor = torch.tensor(adj.toarray(), dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            adj_tensor = adj.to(device)\n",
    "        deg = torch.sum(adj_tensor, dim=1)\n",
    "        m = torch.sum(deg) / 2\n",
    "        d_outer = torch.outer(deg, deg) / (2 * m)\n",
    "        B = adj_tensor - d_outer\n",
    "        modularity_term = torch.trace(C.t() @ B @ C)\n",
    "        mod_loss = -(1 / (2 * m)) * modularity_term\n",
    "        x_norm = X / (X.norm(dim=1, keepdim=True) + 1e-8)\n",
    "        W = torch.mm(x_norm, x_norm.t()).clamp(min=0)\n",
    "        s = torch.sum(W, dim=1)\n",
    "        w = torch.sum(s) / 2\n",
    "        s_outer = torch.outer(s, s) / (2 * w + 1e-8)\n",
    "        B_attr = W - s_outer\n",
    "        attr_mod_term = torch.trace(C.t() @ B_attr @ C)\n",
    "        attr_loss = -(1 / (2 * w + 1e-8)) * attr_mod_term\n",
    "        ones = torch.ones((n, 1), device=device)\n",
    "        cluster_distribution = C.t() @ ones\n",
    "        collapse_term = torch.norm(cluster_distribution - 1, p=1)\n",
    "        collapse_reg = (torch.sqrt(torch.tensor(k, device=device, dtype=torch.float32)) / n) * collapse_term\n",
    "        # Use self.alpha (learnable)\n",
    "        loss = self.alpha * mod_loss + (1 - self.alpha) * attr_loss + collapse_reg\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd822fcb-6411-46cb-8441-3795e4eb176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_clusters, dropout=0.5, leaky_relu_negative_slope=0.2):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.gcn1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.gcn2 = GCNConv(hidden_dim, n_clusters)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leaky_relu = nn.LeakyReLU(leaky_relu_negative_slope)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gcn1(x, edge_index)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        C = F.softmax(x, dim=1)\n",
    "        return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c77f457-957c-49de-884b-b4c2a8b74b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = X.shape[1]\n",
    "hidden_dim = 64  # or desired size\n",
    "dropout = 0.5\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1582bf70-ab23-4c4f-afa5-ccdcd910e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "gcn_model = GCNModel(in_dim, hidden_dim, n_clusters, dropout).to(device)\n",
    "loss_fn = ModularityLoss(n_clusters, initial_alpha=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d5987b-0613-44d4-91d8-35bc52d845ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    list(gcn_model.parameters()) + list(loss_fn.parameters()),\n",
    "    lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5997ffd1-214a-40db-b7a3-edc32789b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_tensor = torch.tensor(adj, dtype=torch.float32)\n",
    "\n",
    "# Ensure norm_adj is tensor and on device\n",
    "if not torch.is_tensor(adj_tensor):\n",
    "    adj_norm = torch.tensor(adj_tensor.toarray(), dtype=torch.float32, device=device)\n",
    "else:\n",
    "    adj_norm = adj_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91e3750a-73ec-432b-b72b-d171e5e26cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move tensors to the device\n",
    "X = X.to(device)\n",
    "C = C.to(device)\n",
    "adj_tensor = torch.tensor(adj, dtype=torch.float32).to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "\n",
    "# Instantiate model and loss on device\n",
    "gcn_model = gcn_model.to(device)\n",
    "loss_fn = loss_fn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc94dcb0-5971-48c3-beb5-113b0fc7946a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss=0.705234706401825, Alpha=0.4997046887874603\n",
      "Epoch 1: Loss=0.7053388357162476, Alpha=0.49955153465270996\n",
      "Epoch 2: Loss=0.7024539113044739, Alpha=0.4993745982646942\n",
      "Epoch 3: Loss=0.705615758895874, Alpha=0.49914586544036865\n",
      "Epoch 4: Loss=0.6980979442596436, Alpha=0.4989511966705322\n",
      "Epoch 5: Loss=0.7070960402488708, Alpha=0.49878063797950745\n",
      "Epoch 6: Loss=0.7053557634353638, Alpha=0.49864742159843445\n",
      "Epoch 7: Loss=0.7070155739784241, Alpha=0.4985295832157135\n",
      "Epoch 8: Loss=0.7041813731193542, Alpha=0.49841776490211487\n",
      "Epoch 9: Loss=0.7025058269500732, Alpha=0.498278945684433\n",
      "Epoch 10: Loss=0.7071056962013245, Alpha=0.4982041120529175\n",
      "Epoch 11: Loss=0.6802989840507507, Alpha=0.4981391131877899\n",
      "Epoch 12: Loss=0.7047457695007324, Alpha=0.49803948402404785\n",
      "Epoch 13: Loss=0.707088828086853, Alpha=0.497982382774353\n",
      "Epoch 14: Loss=0.7025809288024902, Alpha=0.49796923995018005\n",
      "Epoch 15: Loss=0.7066896557807922, Alpha=0.4980018138885498\n",
      "Epoch 16: Loss=0.7064625024795532, Alpha=0.498004674911499\n",
      "Epoch 17: Loss=0.7070741653442383, Alpha=0.4979715049266815\n",
      "Epoch 18: Loss=0.7067890763282776, Alpha=0.4979254901409149\n",
      "Epoch 19: Loss=0.7053742408752441, Alpha=0.49788448214530945\n",
      "Epoch 20: Loss=0.7001651525497437, Alpha=0.49784788489341736\n",
      "Epoch 21: Loss=0.7044031620025635, Alpha=0.49779513478279114\n",
      "Epoch 22: Loss=0.7070456743240356, Alpha=0.49776604771614075\n",
      "Epoch 23: Loss=0.7070966958999634, Alpha=0.49769318103790283\n",
      "Epoch 24: Loss=0.7067370414733887, Alpha=0.49764350056648254\n",
      "Epoch 25: Loss=0.7053568959236145, Alpha=0.49762994050979614\n",
      "Epoch 26: Loss=0.7069032192230225, Alpha=0.49763306975364685\n",
      "Epoch 27: Loss=0.7066265940666199, Alpha=0.4976917505264282\n",
      "Epoch 28: Loss=0.707037627696991, Alpha=0.49776020646095276\n",
      "Epoch 29: Loss=0.7058548927307129, Alpha=0.4977806806564331\n",
      "Epoch 30: Loss=0.7065973281860352, Alpha=0.49782463908195496\n",
      "Epoch 31: Loss=0.7026354074478149, Alpha=0.49785688519477844\n",
      "Epoch 32: Loss=0.7067463994026184, Alpha=0.49788591265678406\n",
      "Epoch 33: Loss=0.7002067565917969, Alpha=0.49793386459350586\n",
      "Epoch 34: Loss=0.7053921222686768, Alpha=0.49796608090400696\n",
      "Epoch 35: Loss=0.7036513090133667, Alpha=0.4980204403400421\n",
      "Epoch 36: Loss=0.6989002227783203, Alpha=0.49803289771080017\n",
      "Epoch 37: Loss=0.6993560194969177, Alpha=0.4980296492576599\n",
      "Epoch 38: Loss=0.6904488801956177, Alpha=0.497997909784317\n",
      "Epoch 39: Loss=0.7066277861595154, Alpha=0.4979495406150818\n",
      "Epoch 40: Loss=0.6866279244422913, Alpha=0.497920423746109\n",
      "Epoch 41: Loss=0.7050691843032837, Alpha=0.49789777398109436\n",
      "Epoch 42: Loss=0.706203281879425, Alpha=0.4978451132774353\n",
      "Epoch 43: Loss=0.6928561925888062, Alpha=0.49781206250190735\n",
      "Epoch 44: Loss=0.7021178007125854, Alpha=0.4977327585220337\n",
      "Epoch 45: Loss=0.7017720341682434, Alpha=0.4976193904876709\n",
      "Epoch 46: Loss=0.6939837336540222, Alpha=0.4975030720233917\n",
      "Epoch 47: Loss=0.701256275177002, Alpha=0.49741247296333313\n",
      "Epoch 48: Loss=0.6896635293960571, Alpha=0.49733075499534607\n",
      "Epoch 49: Loss=0.6910057067871094, Alpha=0.49724292755126953\n",
      "Epoch 50: Loss=0.6860018372535706, Alpha=0.4971495568752289\n",
      "Epoch 51: Loss=0.6776955723762512, Alpha=0.4970511794090271\n",
      "Epoch 52: Loss=0.6826906204223633, Alpha=0.49693426489830017\n",
      "Epoch 53: Loss=0.697616696357727, Alpha=0.4968288540840149\n",
      "Epoch 54: Loss=0.6838421821594238, Alpha=0.496748149394989\n",
      "Epoch 55: Loss=0.6975975036621094, Alpha=0.4967256784439087\n",
      "Epoch 56: Loss=0.6979755163192749, Alpha=0.49675485491752625\n",
      "Epoch 57: Loss=0.6883382797241211, Alpha=0.4968230128288269\n",
      "Epoch 58: Loss=0.6655052900314331, Alpha=0.49688446521759033\n",
      "Epoch 59: Loss=0.6532611846923828, Alpha=0.4969117045402527\n",
      "Epoch 60: Loss=0.7002965211868286, Alpha=0.49695026874542236\n",
      "Epoch 61: Loss=0.6325522065162659, Alpha=0.49709200859069824\n",
      "Epoch 62: Loss=0.6789886951446533, Alpha=0.4972062110900879\n",
      "Epoch 63: Loss=0.639666736125946, Alpha=0.49725446105003357\n",
      "Epoch 64: Loss=0.692559540271759, Alpha=0.4973248243331909\n",
      "Epoch 65: Loss=0.641688346862793, Alpha=0.4973882734775543\n",
      "Epoch 66: Loss=0.6316845417022705, Alpha=0.49739137291908264\n",
      "Epoch 67: Loss=0.6100981831550598, Alpha=0.4974985122680664\n",
      "Epoch 68: Loss=0.6447945833206177, Alpha=0.49749070405960083\n",
      "Epoch 69: Loss=0.6274834871292114, Alpha=0.4973839819431305\n",
      "Epoch 70: Loss=0.6127573847770691, Alpha=0.49728772044181824\n",
      "Epoch 71: Loss=0.6074278950691223, Alpha=0.4972008764743805\n",
      "Epoch 72: Loss=0.5997729301452637, Alpha=0.49712252616882324\n",
      "Epoch 73: Loss=0.6085444688796997, Alpha=0.49700212478637695\n",
      "Epoch 74: Loss=0.5917807817459106, Alpha=0.4967973232269287\n",
      "Epoch 75: Loss=0.6186060309410095, Alpha=0.49666255712509155\n",
      "Epoch 76: Loss=0.5940303206443787, Alpha=0.4964926242828369\n",
      "Epoch 77: Loss=0.5912038087844849, Alpha=0.4963393211364746\n",
      "Epoch 78: Loss=0.5221899747848511, Alpha=0.49610692262649536\n",
      "Epoch 79: Loss=0.6218823790550232, Alpha=0.49594637751579285\n",
      "Epoch 80: Loss=0.5893033742904663, Alpha=0.49580156803131104\n",
      "Epoch 81: Loss=0.579480767250061, Alpha=0.49567094445228577\n",
      "Epoch 82: Loss=0.5180249214172363, Alpha=0.4955531358718872\n",
      "Epoch 83: Loss=0.5973333120346069, Alpha=0.4953989088535309\n",
      "Epoch 84: Loss=0.552270770072937, Alpha=0.4952597916126251\n",
      "Epoch 85: Loss=0.5430251955986023, Alpha=0.4951343238353729\n",
      "Epoch 86: Loss=0.583233654499054, Alpha=0.4949270486831665\n",
      "Epoch 87: Loss=0.5156017541885376, Alpha=0.49483758211135864\n",
      "Epoch 88: Loss=0.5112857818603516, Alpha=0.49485158920288086\n",
      "Epoch 89: Loss=0.4936601221561432, Alpha=0.4949565529823303\n",
      "Epoch 90: Loss=0.5023921728134155, Alpha=0.49505123496055603\n",
      "Epoch 91: Loss=0.4788331985473633, Alpha=0.4951366186141968\n",
      "Epoch 92: Loss=0.5064324736595154, Alpha=0.4952136278152466\n",
      "Epoch 93: Loss=0.5350562334060669, Alpha=0.4952830970287323\n",
      "Epoch 94: Loss=0.40709832310676575, Alpha=0.4950167238712311\n",
      "Epoch 95: Loss=0.4532540440559387, Alpha=0.4947763681411743\n",
      "Epoch 96: Loss=0.5403690338134766, Alpha=0.49455949664115906\n",
      "Epoch 97: Loss=0.4662262201309204, Alpha=0.4944452941417694\n",
      "Epoch 98: Loss=0.4521421790122986, Alpha=0.49418920278549194\n",
      "Epoch 99: Loss=0.4125777781009674, Alpha=0.49395814538002014\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    gcn_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    C = gcn_model(X, edge_index)\n",
    "    loss = loss_fn(C, X, adj_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch}: Loss={loss.item()}, Alpha={loss_fn.alpha.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cc93cc-2905-4455-93bb-61b47e7970f9",
   "metadata": {},
   "source": [
    "## Testing batching mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec43642b-f75f-4ebd-89b1-13098cb4c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mini_batch_sampler(adj, batch_size, num_batches, \n",
    "                                walk_length=4, num_walks=20):\n",
    "    \"\"\"\n",
    "    Efficient mini-batch sampling of nodes using random walk neighborhood expansion.\n",
    "    - adj: normalized adjacency matrix (scipy sparse csr matrix)\n",
    "    - batch_size: number of nodes per batch\n",
    "    - num_batches: number of batches to sample\n",
    "    - walk_length: number of steps per walk (default 4)\n",
    "    - num_walks: number of walks per node (default 20)\n",
    "    Returns a list of numpy arrays for each batch, each array containing the batch nodes + expanded neighbors.\n",
    "    \"\"\"\n",
    "    nnodes = adj.shape[0]\n",
    "    all_nodes = np.arange(nnodes)\n",
    "    batches = []\n",
    "    for _ in range(num_batches):\n",
    "        # 1. Sample batch nodes\n",
    "        batch_nodes = np.random.choice(all_nodes, size=batch_size, replace=False)\n",
    "        # 2. Expand neighborhood by random walks\n",
    "        visited = set(batch_nodes)\n",
    "        for node in batch_nodes:\n",
    "            for _ in range(num_walks):\n",
    "                curr = node\n",
    "                for _ in range(walk_length):\n",
    "                    neighbors = adj[curr].nonzero()[1]\n",
    "                    # print(\"neighbors\", neighbors, neighbors[1])\n",
    "                    if len(neighbors) == 0:\n",
    "                        break\n",
    "                    next_node = np.random.choice(neighbors)\n",
    "                    visited.add(next_node)\n",
    "                    curr = next_node\n",
    "        subgraph_nodes = np.array(list(visited))\n",
    "        print(\"Nodes in current batch: \", subgraph_nodes)\n",
    "        batches.append(subgraph_nodes)\n",
    "    return batches\n",
    "\n",
    "# Example usage:\n",
    "# norm_adj is your normalized adjacency matrix from collect_data\n",
    "# batches = mini_batch_sampler(norm_adj, batch_size=32, num_batches=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0779e012-d5c1-4458-b335-d63ef90fe7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from src.utils import collect_data\n",
    "\n",
    "norm_adj, data, nclasses = collect_data('PubMed')\n",
    "features = data.x.to(device)\n",
    "edge_index = data.edge_index.to(device)\n",
    "labels = data.y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3393fc4-e8e9-4feb-880b-bb000d482a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes in current batch:  [    0 16396  8205 ... 16379  8188  8189]\n",
      "Nodes in current batch:  [    6  8199    12 ... 16373 16375 16376]\n",
      "Nodes in current batch:  [ 8193 16386  8196 ... 16354 16355 16357]\n",
      "Nodes in current batch:  [16384  8193 16389 ... 16365 16370  8186]\n",
      "Nodes in current batch:  [16387     7  8201 ...  8165 16373 16379]\n",
      "Nodes in current batch:  [ 8193     2  8197 ...  8183 16380 16382]\n",
      "Nodes in current batch:  [   17    22    27 ... 16373 16378 16379]\n",
      "Nodes in current batch:  [    6     7  8203 ...  8183 16380 16381]\n",
      "Nodes in current batch:  [16384  8192 16387 ... 16380 16381  8191]\n",
      "Nodes in current batch:  [ 8192     1 16387 ...  8173  8175 16380]\n",
      "Nodes in current batch:  [    0  8192  8198 ... 16376 16378  8188]\n",
      "Nodes in current batch:  [16389     6  8198 ... 16342  8153 16355]\n",
      "Nodes in current batch:  [    0     1 16387 ... 16370 16380  8191]\n",
      "Nodes in current batch:  [ 8192 16387     6 ... 16362  8176 16372]\n",
      "Nodes in current batch:  [16384  8196     4 ...  8172 16370  8186]\n",
      "Nodes in current batch:  [ 8192 16384     2 ... 16375 16378 16383]\n",
      "Nodes in current batch:  [16384     0  8193 ...  8182 16378  8191]\n",
      "Nodes in current batch:  [16387 16389  8197 ... 16362  8190 16383]\n",
      "Nodes in current batch:  [ 8192 16384 16387 ... 16342  8154 16376]\n",
      "Nodes in current batch:  [16385  8193     5 ... 16364 16369 16375]\n",
      "Nodes in current batch:  [ 8192 16386 16387 ... 16365  8181  8183]\n",
      "Nodes in current batch:  [16384  8192     6 ... 16371  8180 16375]\n",
      "Nodes in current batch:  [ 8194     4  8198 ... 16373 16380 16383]\n",
      "Nodes in current batch:  [ 8194     3  8196 ...  8183 16379 16383]\n",
      "Nodes in current batch:  [ 8198     7 16398 ... 16358 16364 16383]\n",
      "Nodes in current batch:  [    0 16384  8197 ...  8183 16376 16381]\n",
      "Nodes in current batch:  [ 8198 16391 16398 ... 16378 16379  8191]\n",
      "Nodes in current batch:  [ 8192  8193  8194 ... 16371 16378 16380]\n",
      "Nodes in current batch:  [16384  8197     6 ... 16361 16366  8191]\n",
      "Nodes in current batch:  [16384 16387     7 ... 16370 16372 16383]\n",
      "Nodes in current batch:  [ 8192 16386 16387 ... 16346  8175 16376]\n",
      "Nodes in current batch:  [16387  8197  8199 ...  8175 16371 16379]\n",
      "Nodes in current batch:  [    6     9  8205 ...  8175 16376 16379]\n",
      "Nodes in current batch:  [    0     2  8198 ... 16368  8177 16370]\n",
      "Nodes in current batch:  [16384     2  8197 ... 16347  8159 16355]\n",
      "Nodes in current batch:  [16387     6  8198 ... 16367 16373 16380]\n",
      "Nodes in current batch:  [ 8192     5     7 ... 16377 16380 16383]\n",
      "Nodes in current batch:  [16384     5     6 ... 16368  8185 16381]\n",
      "Nodes in current batch:  [ 8193  8197     5 ...  8181 16378 16379]\n",
      "Nodes in current batch:  [16384  8192 16389 ... 16355  8185 16380]\n",
      "Nodes in current batch:  [ 8193     2     9 ...  8175  8183 16380]\n",
      "Nodes in current batch:  [ 8198     6 16397 ...  8178 16373 16378]\n",
      "Nodes in current batch:  [    0 16386 16387 ... 16371 16378 16383]\n",
      "Nodes in current batch:  [    0  8194     6 ... 16376 16379 16383]\n",
      "Nodes in current batch:  [16384  8192  8197 ... 16370 16372  8182]\n",
      "Nodes in current batch:  [16384 16385 16387 ...  8173 16366 16376]\n",
      "Nodes in current batch:  [ 8192  8197  8198 ... 16357 16360 16379]\n",
      "Nodes in current batch:  [ 8192     6  8211 ... 16375 16378 16379]\n",
      "Nodes in current batch:  [ 8192 16387  8198 ... 16363  8175 16379]\n",
      "Nodes in current batch:  [16384  8195     6 ... 16366 16370 16383]\n",
      "Nodes in current batch:  [    0  8195     7 ... 16361 16365 16380]\n",
      "Nodes in current batch:  [ 8192 16387 16390 ...  8187 16380 16382]\n",
      "Nodes in current batch:  [16384 16387  8195 ...  8186 16379 16380]\n",
      "Nodes in current batch:  [16384  8198     7 ... 16378 16379  8190]\n",
      "Nodes in current batch:  [16384 16387 16389 ... 16366 16373  8182]\n",
      "Nodes in current batch:  [16387     6     7 ...  8164  8180 16372]\n",
      "Nodes in current batch:  [16385     7    14 ... 16380 16381  8191]\n",
      "Nodes in current batch:  [    5  8198     9 ... 16361 16365  8186]\n",
      "Nodes in current batch:  [ 8192 16385 16384 ... 16371  8184 16378]\n",
      "Nodes in current batch:  [ 8192     2 16387 ... 16376 16378 16380]\n",
      "Nodes in current batch:  [ 8192 16384     2 ...  8177 16370 16379]\n",
      "Nodes in current batch:  [16384  8192  8196 ... 16367 16379 16383]\n",
      "Nodes in current batch:  [ 8193  8195  8197 ... 16363 16372  8187]\n",
      "Nodes in current batch:  [    7 16393    10 ...  8169 16367  8177]\n"
     ]
    }
   ],
   "source": [
    "batches = mini_batch_sampler(norm_adj, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b13112a0-6d81-44a8-9ea5-c54e60a70196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([    0, 16396,  8205, ..., 16379,  8188,  8189]),\n",
       " array([    6,  8199,    12, ..., 16373, 16375, 16376]),\n",
       " array([ 8193, 16386,  8196, ..., 16354, 16355, 16357]),\n",
       " array([16384,  8193, 16389, ..., 16365, 16370,  8186]),\n",
       " array([16387,     7,  8201, ...,  8165, 16373, 16379]),\n",
       " array([ 8193,     2,  8197, ...,  8183, 16380, 16382]),\n",
       " array([   17,    22,    27, ..., 16373, 16378, 16379]),\n",
       " array([    6,     7,  8203, ...,  8183, 16380, 16381]),\n",
       " array([16384,  8192, 16387, ..., 16380, 16381,  8191]),\n",
       " array([ 8192,     1, 16387, ...,  8173,  8175, 16380]),\n",
       " array([    0,  8192,  8198, ..., 16376, 16378,  8188]),\n",
       " array([16389,     6,  8198, ..., 16342,  8153, 16355]),\n",
       " array([    0,     1, 16387, ..., 16370, 16380,  8191]),\n",
       " array([ 8192, 16387,     6, ..., 16362,  8176, 16372]),\n",
       " array([16384,  8196,     4, ...,  8172, 16370,  8186]),\n",
       " array([ 8192, 16384,     2, ..., 16375, 16378, 16383]),\n",
       " array([16384,     0,  8193, ...,  8182, 16378,  8191]),\n",
       " array([16387, 16389,  8197, ..., 16362,  8190, 16383]),\n",
       " array([ 8192, 16384, 16387, ..., 16342,  8154, 16376]),\n",
       " array([16385,  8193,     5, ..., 16364, 16369, 16375]),\n",
       " array([ 8192, 16386, 16387, ..., 16365,  8181,  8183]),\n",
       " array([16384,  8192,     6, ..., 16371,  8180, 16375]),\n",
       " array([ 8194,     4,  8198, ..., 16373, 16380, 16383]),\n",
       " array([ 8194,     3,  8196, ...,  8183, 16379, 16383]),\n",
       " array([ 8198,     7, 16398, ..., 16358, 16364, 16383]),\n",
       " array([    0, 16384,  8197, ...,  8183, 16376, 16381]),\n",
       " array([ 8198, 16391, 16398, ..., 16378, 16379,  8191]),\n",
       " array([ 8192,  8193,  8194, ..., 16371, 16378, 16380]),\n",
       " array([16384,  8197,     6, ..., 16361, 16366,  8191]),\n",
       " array([16384, 16387,     7, ..., 16370, 16372, 16383]),\n",
       " array([ 8192, 16386, 16387, ..., 16346,  8175, 16376]),\n",
       " array([16387,  8197,  8199, ...,  8175, 16371, 16379]),\n",
       " array([    6,     9,  8205, ...,  8175, 16376, 16379]),\n",
       " array([    0,     2,  8198, ..., 16368,  8177, 16370]),\n",
       " array([16384,     2,  8197, ..., 16347,  8159, 16355]),\n",
       " array([16387,     6,  8198, ..., 16367, 16373, 16380]),\n",
       " array([ 8192,     5,     7, ..., 16377, 16380, 16383]),\n",
       " array([16384,     5,     6, ..., 16368,  8185, 16381]),\n",
       " array([ 8193,  8197,     5, ...,  8181, 16378, 16379]),\n",
       " array([16384,  8192, 16389, ..., 16355,  8185, 16380]),\n",
       " array([ 8193,     2,     9, ...,  8175,  8183, 16380]),\n",
       " array([ 8198,     6, 16397, ...,  8178, 16373, 16378]),\n",
       " array([    0, 16386, 16387, ..., 16371, 16378, 16383]),\n",
       " array([    0,  8194,     6, ..., 16376, 16379, 16383]),\n",
       " array([16384,  8192,  8197, ..., 16370, 16372,  8182]),\n",
       " array([16384, 16385, 16387, ...,  8173, 16366, 16376]),\n",
       " array([ 8192,  8197,  8198, ..., 16357, 16360, 16379]),\n",
       " array([ 8192,     6,  8211, ..., 16375, 16378, 16379]),\n",
       " array([ 8192, 16387,  8198, ..., 16363,  8175, 16379]),\n",
       " array([16384,  8195,     6, ..., 16366, 16370, 16383]),\n",
       " array([    0,  8195,     7, ..., 16361, 16365, 16380]),\n",
       " array([ 8192, 16387, 16390, ...,  8187, 16380, 16382]),\n",
       " array([16384, 16387,  8195, ...,  8186, 16379, 16380]),\n",
       " array([16384,  8198,     7, ..., 16378, 16379,  8190]),\n",
       " array([16384, 16387, 16389, ..., 16366, 16373,  8182]),\n",
       " array([16387,     6,     7, ...,  8164,  8180, 16372]),\n",
       " array([16385,     7,    14, ..., 16380, 16381,  8191]),\n",
       " array([    5,  8198,     9, ..., 16361, 16365,  8186]),\n",
       " array([ 8192, 16385, 16384, ..., 16371,  8184, 16378]),\n",
       " array([ 8192,     2, 16387, ..., 16376, 16378, 16380]),\n",
       " array([ 8192, 16384,     2, ...,  8177, 16370, 16379]),\n",
       " array([16384,  8192,  8196, ..., 16367, 16379, 16383]),\n",
       " array([ 8193,  8195,  8197, ..., 16363, 16372,  8187]),\n",
       " array([    7, 16393,    10, ...,  8169, 16367,  8177])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5896c462-2b81-421c-b9e6-d360f1f9c186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19717"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnodes = norm_adj.shape[0]\n",
    "nnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f569b422-3d2c-499d-877f-1fcf7b68372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = np.arange(nnodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0de4f922-2377-49a5-a04d-63d270d5d11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 19714, 19715, 19716])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c45d1d-7ec0-4658-8154-ca73572ca625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
